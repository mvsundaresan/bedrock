{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97572c6f-0d1a-4ee5-8e01-651a05c8e031",
   "metadata": {},
   "source": [
    "# Large File Summarization using LangChain/LCEL with Bedrock API \n",
    "## GenAI Code Accelerator \n",
    "Author: Sundaresan Manoharan - Enterprise Architecture AI/ML Team\n",
    "> *This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*\n",
    "\n",
    "Text summarization in Natural Language Processing (NLP) is the process of breaking down large texts into smaller parts. It uses deep learning and machine learning models to extract important information while preserving the meaning of the text from a text document and presenting it in a concise and coherent format. It allows digesting and distilling the essence from large volumes of content efficiently. It is a key capability of LLMs with many potential applications across industries to improve understanding and save time. This notebook demostrates text summarization using Amazon Bedrock API. \n",
    "\n",
    "Challenge: A key challenge is managing large documents that exceed the token limit. Another is obtaining high quality summaries. When we work with large documents, we can face some challenges as the input text might not fit into the model context length, or the model hallucinates with large documents, or, out of memory errors, etc.\n",
    "\n",
    "To solve those problems, we are going to show an architecture that is based on the concept of chunking and chaining prompts. This architecture is leveraging LangChain which is a popular framework for developing applications powered by language models.\n",
    "\n",
    "Use Cases:\n",
    "- Books, Articles, Blogs, Research Papers\n",
    "\n",
    "Foundation Model(s):\n",
    "- Amazon Titan Large\n",
    "- Meta LLaMa 13B\n",
    "\n",
    "This notebook introduces Text Summarization using Amazon Bedrock API.  \n",
    "- Uses various Foundation Models (LLM agnostic)\n",
    "- Uses a PDF document (Earnings Call Transcript, Business/Financial Reports)\n",
    "- Uses simple and easy to adapt bite size'd code accelerator\n",
    "\n",
    "\n",
    "Insert Architecture Diagram\n",
    "\n",
    "In this architecture:\n",
    "\n",
    "1. A large document (or a giant file appending small ones) is loaded\n",
    "1. Langchain utility is used to split it into multiple smaller chunks (chunking)\n",
    "1. First chunk is sent to the model; Model returns the corresponding summary\n",
    "1. Langchain gets next chunk and appends it to the returned summary and sends the combined text as a new request to the model; the process repeats until all chunks are processed\n",
    "1. In the end, you have final summary based on entire content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83862a34-d128-4235-a291-14ba2f4595db",
   "metadata": {},
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25413009-3494-4bb5-bff3-50c17bdb0064",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.3.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "908a8cf7-57fc-4fe9-a2a0-bf55864b8fac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3>=1.28.57\n",
      "  Using cached boto3-1.34.25-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting awscli>=1.29.57\n",
      "  Using cached awscli-1.32.25-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting botocore>=1.31.57\n",
      "  Using cached botocore-1.34.25-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.28.57)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.28.57)\n",
      "  Using cached s3transfer-0.10.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting docutils<0.17,>=0.10 (from awscli>=1.29.57)\n",
      "  Using cached docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
      "Collecting PyYAML<6.1,>=3.10 (from awscli>=1.29.57)\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting colorama<0.4.5,>=0.2.5 (from awscli>=1.29.57)\n",
      "  Using cached colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting rsa<4.8,>=3.1.2 (from awscli>=1.29.57)\n",
      "  Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore>=1.31.57)\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting urllib3<2.1,>=1.25.4 (from botocore>=1.31.57)\n",
      "  Using cached urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting six>=1.5 (from python-dateutil<3.0.0,>=2.1->botocore>=1.31.57)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<4.8,>=3.1.2->awscli>=1.29.57)\n",
      "  Using cached pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached boto3-1.34.25-py3-none-any.whl (139 kB)\n",
      "Using cached awscli-1.32.25-py3-none-any.whl (4.3 MB)\n",
      "Using cached botocore-1.34.25-py3-none-any.whl (11.9 MB)\n",
      "Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "Using cached s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
      "Using cached urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "Using cached pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Installing collected packages: urllib3, six, PyYAML, pyasn1, jmespath, docutils, colorama, rsa, python-dateutil, botocore, s3transfer, boto3, awscli\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.7\n",
      "    Uninstalling urllib3-2.0.7:\n",
      "      Successfully uninstalled urllib3-2.0.7\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: pyasn1\n",
      "    Found existing installation: pyasn1 0.5.1\n",
      "    Uninstalling pyasn1-0.5.1:\n",
      "      Successfully uninstalled pyasn1-0.5.1\n",
      "  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 1.0.1\n",
      "    Uninstalling jmespath-1.0.1:\n",
      "      Successfully uninstalled jmespath-1.0.1\n",
      "  Attempting uninstall: docutils\n",
      "    Found existing installation: docutils 0.16\n",
      "    Uninstalling docutils-0.16:\n",
      "      Successfully uninstalled docutils-0.16\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.4\n",
      "    Uninstalling colorama-0.4.4:\n",
      "      Successfully uninstalled colorama-0.4.4\n",
      "  Attempting uninstall: rsa\n",
      "    Found existing installation: rsa 4.7.2\n",
      "    Uninstalling rsa-4.7.2:\n",
      "      Successfully uninstalled rsa-4.7.2\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.34.25\n",
      "    Uninstalling botocore-1.34.25:\n",
      "      Successfully uninstalled botocore-1.34.25\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.10.0\n",
      "    Uninstalling s3transfer-0.10.0:\n",
      "      Successfully uninstalled s3transfer-0.10.0\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.34.25\n",
      "    Uninstalling boto3-1.34.25:\n",
      "      Successfully uninstalled boto3-1.34.25\n",
      "  Attempting uninstall: awscli\n",
      "    Found existing installation: awscli 1.32.25\n",
      "    Uninstalling awscli-1.32.25:\n",
      "      Successfully uninstalled awscli-1.32.25\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.4 which is incompatible.\n",
      "jupyterlab 3.4.4 requires jupyter-server~=1.16, but you have jupyter-server 2.12.1 which is incompatible.\n",
      "jupyterlab-server 2.10.3 requires jupyter-server~=1.4, but you have jupyter-server 2.12.1 which is incompatible.\n",
      "notebook 6.5.6 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.6.0 which is incompatible.\n",
      "notebook 6.5.6 requires pyzmq<25,>=17, but you have pyzmq 25.1.2 which is incompatible.\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 3.3.2 which is incompatible.\n",
      "pyasn1-modules 0.2.8 requires pyasn1<0.5.0,>=0.4.6, but you have pyasn1 0.5.1 which is incompatible.\n",
      "sagemaker 2.199.0 requires urllib3<1.27, but you have urllib3 2.0.7 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.18.1 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.2 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires ipython<8,>=7.31.1; python_version >= \"3\", but you have ipython 8.18.1 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires jupyter-client<8,>=7.3.4; python_version >= \"3\", but you have jupyter-client 8.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyYAML-6.0.1 awscli-1.32.25 boto3-1.34.25 botocore-1.34.25 colorama-0.4.4 docutils-0.16 jmespath-1.0.1 pyasn1-0.5.1 python-dateutil-2.8.2 rsa-4.7.2 s3transfer-0.10.0 six-1.16.0 urllib3-2.0.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10917cd1-6084-48d6-a016-2109de2f44cb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.14 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.0.14)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.14 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.1.14)\n",
      "Requirement already satisfied: langsmith<0.0.84,>=0.0.83 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.0.83)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.10.13)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.14->langchain) (3.5.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.14->langchain) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (1.1.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.14->langchain) (1.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.37.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%pip install langchain\n",
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205669b5-9fe7-4885-a716-428640308a13",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12c074c5-dbd7-4396-9ced-c93d57ef8e46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "from IPython.display import display_markdown, Markdown, clear_output\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f892a385-2458-40c8-9416-6d74819a9d4a",
   "metadata": {},
   "source": [
    "### Initialize boto session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ed5c71c-3a7e-421c-becc-ee9336e7a6d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "# module_path = \"..\"\n",
    "# sys.path.append(os.path.abspath(module_path))\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "aws_region = boto_session.region_name\n",
    "print(aws_region)\n",
    "br_client = boto_session.client(\"bedrock\", region_name=aws_region)\n",
    "br_runtime = boto_session.client(\"bedrock-runtime\", region_name=aws_region)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd61b405-d024-430b-8430-eb96866914e2",
   "metadata": {},
   "source": [
    "### Test Connection & List Foundation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fa1dbd9-35b6-4cdf-aeef-0d5e41391beb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelArn</th>\n",
       "      <th>modelId</th>\n",
       "      <th>modelName</th>\n",
       "      <th>providerName</th>\n",
       "      <th>inputModalities</th>\n",
       "      <th>outputModalities</th>\n",
       "      <th>responseStreamingSupported</th>\n",
       "      <th>customizationsSupported</th>\n",
       "      <th>inferenceTypesSupported</th>\n",
       "      <th>modelLifecycle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/am...</td>\n",
       "      <td>amazon.titan-tg1-large</td>\n",
       "      <td>Titan Text Large</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/am...</td>\n",
       "      <td>amazon.titan-image-generator-v1:0</td>\n",
       "      <td>Titan Image Generator G1</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>[TEXT, IMAGE]</td>\n",
       "      <td>[IMAGE]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[FINE_TUNING]</td>\n",
       "      <td>[ON_DEMAND, PROVISIONED]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/am...</td>\n",
       "      <td>amazon.titan-image-generator-v1</td>\n",
       "      <td>Titan Image Generator G1</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>[TEXT, IMAGE]</td>\n",
       "      <td>[IMAGE]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/am...</td>\n",
       "      <td>amazon.titan-embed-g1-text-02</td>\n",
       "      <td>Titan Text Embeddings v2</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[EMBEDDING]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/am...</td>\n",
       "      <td>amazon.titan-text-lite-v1:0:4k</td>\n",
       "      <td>Titan Text G1 - Lite</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[FINE_TUNING, CONTINUED_PRE_TRAINING]</td>\n",
       "      <td>[PROVISIONED]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            modelArn  \\\n",
       "0  arn:aws:bedrock:us-east-1::foundation-model/am...   \n",
       "1  arn:aws:bedrock:us-east-1::foundation-model/am...   \n",
       "2  arn:aws:bedrock:us-east-1::foundation-model/am...   \n",
       "3  arn:aws:bedrock:us-east-1::foundation-model/am...   \n",
       "4  arn:aws:bedrock:us-east-1::foundation-model/am...   \n",
       "\n",
       "                             modelId                 modelName providerName  \\\n",
       "0             amazon.titan-tg1-large          Titan Text Large       Amazon   \n",
       "1  amazon.titan-image-generator-v1:0  Titan Image Generator G1       Amazon   \n",
       "2    amazon.titan-image-generator-v1  Titan Image Generator G1       Amazon   \n",
       "3      amazon.titan-embed-g1-text-02  Titan Text Embeddings v2       Amazon   \n",
       "4     amazon.titan-text-lite-v1:0:4k      Titan Text G1 - Lite       Amazon   \n",
       "\n",
       "  inputModalities outputModalities responseStreamingSupported  \\\n",
       "0          [TEXT]           [TEXT]                       True   \n",
       "1   [TEXT, IMAGE]          [IMAGE]                        NaN   \n",
       "2   [TEXT, IMAGE]          [IMAGE]                        NaN   \n",
       "3          [TEXT]      [EMBEDDING]                        NaN   \n",
       "4          [TEXT]           [TEXT]                       True   \n",
       "\n",
       "                 customizationsSupported   inferenceTypesSupported  \\\n",
       "0                                     []               [ON_DEMAND]   \n",
       "1                          [FINE_TUNING]  [ON_DEMAND, PROVISIONED]   \n",
       "2                                     []               [ON_DEMAND]   \n",
       "3                                     []               [ON_DEMAND]   \n",
       "4  [FINE_TUNING, CONTINUED_PRE_TRAINING]             [PROVISIONED]   \n",
       "\n",
       "         modelLifecycle  \n",
       "0  {'status': 'ACTIVE'}  \n",
       "1  {'status': 'ACTIVE'}  \n",
       "2  {'status': 'ACTIVE'}  \n",
       "3  {'status': 'ACTIVE'}  \n",
       "4  {'status': 'ACTIVE'}  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fms = br_client.list_foundation_models()['modelSummaries']\n",
    "dfFM = pd.DataFrame(fms)\n",
    "print(dfFM.shape)\n",
    "dfFM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d40e865-f417-4dcd-ad88-72fe78253a60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['modelArn', 'modelId', 'modelName', 'providerName', 'inputModalities',\n",
       "       'outputModalities', 'responseStreamingSupported',\n",
       "       'customizationsSupported', 'inferenceTypesSupported', 'modelLifecycle'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfFM.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b147e97-1057-42b6-87ac-355fd743158b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Titan Text Large', 'Titan Image Generator G1',\n",
       "       'Titan Text Embeddings v2', 'Titan Text G1 - Lite',\n",
       "       'Titan Text G1 - Express', 'Titan Embeddings G1 - Text',\n",
       "       'Titan Multimodal Embeddings G1', 'SDXL 0.8', 'SDXL 1.0',\n",
       "       'J2 Grande Instruct', 'J2 Jumbo Instruct', 'Jurassic-2 Mid',\n",
       "       'Jurassic-2 Ultra', 'Claude Instant', 'Claude', 'Command',\n",
       "       'Command Light', 'Embed English', 'Embed Multilingual',\n",
       "       'Llama 2 Chat 13B', 'Llama 2 Chat 70B', 'Llama 2 13B',\n",
       "       'Llama 2 70B'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfFM.modelName.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f88ed0-d42a-4fe0-ab9a-b9119a2d492c",
   "metadata": {},
   "source": [
    "## Summarize long text \n",
    "\n",
    "### Configuring LangChain with Boto3\n",
    "\n",
    "LangChain allows you to access Bedrock once you pass boto3 session information to LangChain. If you pass None as the boto3 session information to LangChain, LangChain tries to get session information from your environment.\n",
    "In order to ensure the right client is used we are going to instantiate one thanks to a utility method.\n",
    "\n",
    "You need to specify LLM for LangChain Bedrock class, and can pass arguments for inference. Here you specify Amazon Titan Text Large in `model_id` and pass Titan's inference parameter in `textGenerationConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abf14762-1821-42e9-894c-fa7686837390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelId = \"amazon.titan-tg1-large\"\n",
    "llm = Bedrock(\n",
    "    model_id=modelId,\n",
    "    model_kwargs={\n",
    "        \"maxTokenCount\": 4096,\n",
    "        \"stopSequences\": [],\n",
    "        \"temperature\": 0,\n",
    "        \"topP\": 1,\n",
    "    },\n",
    "    client=br_runtime,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2daa91e-187a-4272-a2ef-1e719ddc2f5d",
   "metadata": {},
   "source": [
    "### Download a public dataset\n",
    "\n",
    "### Loading a text file with many tokens\n",
    "\n",
    "In letters directory, you can find a text file of Amazon's CEO letter to shareholders in 2022. The following cell loads the text file and counts the number of tokens in the file.\n",
    "\n",
    "You will see warning indicating the number of tokens in the text file exceeeds the maximum number of tokens for this model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9cb362d-87d3-4c65-8c60-3d1751fb6be0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6526 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6526"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%sh\n",
    "\n",
    "# wget -O fannie-mf-commentary-oct-2023.pdf https://www.fanniemae.com/media/49331/display\n",
    "\n",
    "shareholder_letter = \"./2022-letter.txt\"\n",
    "\n",
    "with open(shareholder_letter, \"r\") as file:\n",
    "    letter = file.read()\n",
    "    \n",
    "llm.get_num_tokens(letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d880c992-c1c4-4cc6-af4b-03839b8fb6ca",
   "metadata": {},
   "source": [
    "### Splitting the long text into chunks\n",
    "\n",
    "The text is too long to fit in the prompt, so we will split it into smaller chunks. RecursiveCharacterTextSplitter in LangChain supports splitting long text into chunks recursively until size of each chunk becomes smaller than chunk_size. A text is separated with separators=[\"\\n\\n\", \"\\n\"] into chunks, which avoids splitting each paragraph into multiple chunks.\n",
    "\n",
    "Using 4,000 characters per chunk, we can get summaries for each portion separately. The number of tokens, or word pieces, in a chunk depends on the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "629d0c5d-a168-4b14-a3bf-8a68a11be9f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\"], chunk_size=4000, chunk_overlap=100\n",
    ")\n",
    "\n",
    "docs = text_splitter.create_documents([letter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f758151-9608-413c-b882-6e554103f1dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we have 10 documents and the first one has 439 tokens\n"
     ]
    }
   ],
   "source": [
    "num_docs = len(docs)\n",
    "\n",
    "num_tokens_first_doc = llm.get_num_tokens(docs[0].page_content)\n",
    "\n",
    "print(\n",
    "    f\"Now we have {num_docs} documents and the first one has {num_tokens_first_doc} tokens\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4c27b9-effc-4ac0-af9b-b392d887858a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Summarizing chunks and combining them\n",
    "\n",
    "Assuming that the number of tokens is consistent in the other docs we should be good to go. Let's use LangChain's [load_summarize_chain](https://python.langchain.com/en/latest/use_cases/summarization.html) to summarize the text. `load_summarize_chain` provides three ways of summarization: `stuff`, `map_reduce`, and `refine`. \n",
    "- `stuff` puts all the chunks into one prompt. Thus, this would hit the maximum limit of tokens.\n",
    "- `map_reduce` summarizes each chunk, combines the summary, and summarizes the combined summary. If the combined summary is too large, it would raise error.\n",
    "- `refine` summarizes the first chunk, and then summarizes the second chunk with the first summary. The same process repeats until all chunks are summarized.\n",
    "\n",
    "`map_reduce` and `refine` invoke LLM multiple times and takes time for obtaining final summary. \n",
    "Let's try `map_reduce` here. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4d5220-5739-4c3c-9840-1a5be8bdcac9",
   "metadata": {},
   "source": [
    "### Option 1. Use Map reduce pattern on Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6f51611-9506-4836-9cac-c74948a102a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set verbose=True if you want to see the prompts being used\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "summary_chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ece7126a-165d-45a9-a243-e0c55656fb02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output = \"\"\n",
    "\n",
    "try:\n",
    "    output = summary_chain.run(docs)\n",
    "except ValueError as error:\n",
    "    raise error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91033929-1e11-48b3-9d26-0ece4e8420ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jeff Bezos, the CEO of Amazon, remains positive and enthusiastic about the company's future despite the challenging macroeconomic environment in 2022. Amazon has increased demand, innovated in its largest businesses, and made adjustments to its investment decisions. The company has undergone constant change in its 25 years, from being a books-only retailer to selling nearly every physical and digital retail item and building a business around technology infrastructure services in the cloud. Amazon has taken a deep look across the company, business by business, invention by invention, and asked themselves whether they had conviction about each initiative's long-term potential to drive enough revenue, operating income, free cash flow, and return on invested capital. This has led to the closure of certain businesses, the amendment of programs, and the reprioritization of resources.\n",
      "\n",
      "Amazon has announced that corporate employees will be required to return to the office at least three days a week starting in May. The company believes that collaborating and inventing is easier and more effective when done in person, and many of Amazon's best inventions have come from serendipitous interactions in the office. The rising cost to serve in the Stores fulfillment network has been a critical challenge, and Amazon has made several changes to improve fulfillment costs and speed of delivery. During the early part of the pandemic, the consumer business grew at an extraordinary clip, doubling the fulfillment center footprint and substantially accelerating building a last-mile transportation network. Amazon has scrutinized every process path in its fulfillment centers and transportation network and redesigned scores of processes and mechanisms, resulting in steady productivity gains and cost reductions over the last few quarters.\n",
      "\n",
      "Amazon has made significant structural changes to its US fulfillment network to deliver lower costs and faster speed. It has moved from a national fulfillment network to a regionalized network model with eight interconnected regions in smaller geographic areas. AWS has an $85B annualized revenue run rate and is still early in its adoption curve, but it faces short-term headwinds due to companies being more cautious in spending. AWS is taking a different tack, focusing on building customer relationships and a business that outlasts all of us. Its sales and support teams are helping customers optimize their AWS spend so they can better weather this uncertain economy.\n",
      "\n",
      "AWS faces short-term headwinds, but it maintains strong fundamentals with a robust customer pipeline, active migrations, and enterprises opting for AWS for its agility, innovation, cost-efficiency, and security benefits. The company continues to deliver new capabilities rapidly and invest in long-term inventions, such as its Graviton2-based compute instances and Graviton3 chips for machine learning training and inference. Amazon's Advertising business is also uniquely effective for brands, tailoring sponsored products to be relevant to customer search queries and resulting in more useful and effective advertising. Despite slowing growth in the overall advertising industry, Amazon's Advertising revenue has continued to grow rapidly, with 23% YoY growth in Q4 2022 and 25% YoY growth overall for 2022.\n",
      "\n",
      "Amazon is committed to being the best place for advertisers to build their brands and has invested in machine learning and planning and measurement solutions to help marketers gain insight into advertising effectiveness. Amazon Marketing Cloud (AMC) is a secure digital environment where advertisers can run custom audience and campaign analytics in a privacy-safe manner. The Advertising and AWS teams have collaborated to enable companies to store their data in AWS, operate securely in AMC, perform analytics in AWS, and activate advertising on Amazon or third-party publishers through the Amazon Demand-Side Platform. Amazon is also looking at new investment opportunities, such as expanding from just selling Books to adding categories like Music, Video, Electronics, and Toys, and investing in new international geographies, such as India, Brazil, Mexico, Australia, various European countries, the Middle East, and parts of Africa. These investments will allow Amazon to help more customers across the world and build a larger free cash flow-generating consumer business.\n",
      "\n",
      "Amazon has been expanding its customer offerings across large, unique product retail market segments, including grocery. The grocery market is an $800B market segment in the US, with the average household shopping three to four times per week. Amazon has built a significant grocery business over nearly 20 years, offering more than three million items compared to a typical supermarket's 30K. To serve more of its customers' grocery needs, Amazon needs a broader physical store footprint. Whole Foods Market pioneered the natural and organic specialty grocery store concept 40 years ago and is a large and growing business. Amazon Fresh is the brand they've been experimenting with for a few years and are working hard to identify and build the right mass grocery format for Amazon scale. Amazon Business is another example of an investment where their ecommerce and logistics capabilities position them well to pursue this large market segment. It allows businesses, municipalities, and organizations to procure products like office supplies and other bulk items easily and at great savings.\n",
      "\n",
      "Amazon has developed Buy with Prime to assist third-party brands and sellers in offering their products on their websites to Amazon Prime members, providing fast, free Prime shipping, and seamless checkout. It has increased shopper conversion on third-party shopping sites by 25% on average and is now available to all US merchants. Amazon is also expanding internationally, pursuing large retail market segments, and using its unique assets to help merchants sell more effectively on their websites. In 2003, AWS was a classic example of an investment that was further from its core businesses. Amazon Healthcare and Kuiper are potential analogues.\n",
      "\n",
      "Amazon Pharmacy is a full-service, online pharmacy that offers transparent pricing, easy refills, and savings for Prime members. However, customers have expressed a strong desire for Amazon to provide a better alternative to the inefficient and unsatisfying broader healthcare experience. In July 2022, Amazon announced its acquisition of One Medical, a patient-focused experience with a digital app, offices in cities across the US, and relationships with specialty physicians. One Medical and Amazon will continue to innovate together to change what primary care will look like for customers.\n",
      "\n",
      "Amazon is developing Kuiper, a low-Earth orbit satellite system to deliver high-quality broadband internet service to places around the world that don't currently have it. The system will deliver not only accessibility but affordability, with low-cost antennas that will lower the barriers to access and deliver speeds up to 400 megabits per second. Amazon is preparing to launch two prototype satellites to test the entire end-to-end communications network this year and plan to be in beta with commercial customers in 2024. Kuiper represents a large potential opportunity for Amazon, with a large prospective consumer, enterprise, and government customer base, significant revenue and operating profit potential, and relatively few companies with the technical and inventive aptitude to go after it.\n",
      "\n",
      "Amazon is investing heavily in Large Language Models (LLMs) and Generative AI to invent in every area of its business for many decades to come. Machine learning has been a technology with high promise for several decades, but it has only been the last five to ten years that it has started to be used more pervasively by companies. Generative AI is based on very Large Language Models (trained on up to hundreds of billions of parameters, and growing), across expansive datasets, and has radically general and broad recall and learning capabilities. AWS is offering the most price-performant machine learning chips in Trainium and Inferentia so small and large companies can afford to train and run their LLMs in production.\n",
      "\n",
      "LLMs and Generative AI are going to be a big deal for customers, our shareholders, and Amazon. In 2022, the consumer business is $434B, and the vast majority of total market segment share in global retail still resides in physical stores. Global IT spending is $80B in 2022, with about 90% of Global IT spending still on-premises and yet to migrate to the cloud. As these equations steadily flip, Amazon believes its leading customer experiences, relentless invention, customer focus, and hard work will result in significant growth in the coming years. The author believes that their best days are in front of them and looks forward to working with their teammates at Amazon to make it so.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948cba0e-9095-464a-bf1d-10a83ec0ed48",
   "metadata": {},
   "source": [
    "### LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbfce43d-e079-458f-bc77-583d2a168ef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import XMLOutputParser, PydanticOutputParser\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "xml_parser = XMLOutputParser(tags=['insight'])\n",
    "str_parser = StrOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    \n",
    "    Human:\n",
    "    {instructions} : \\\"{document}\\\"\n",
    "    Format help: {format_instructions}.\n",
    "    Assistant:\"\"\",\n",
    "    input_variables=[\"instructions\",\"document\"],\n",
    "    partial_variables={\"format_instructions\": xml_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "insight_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3851efd4-cb66-4f92-97c4-4ebe017c94fd",
   "metadata": {},
   "source": [
    "### Option 2. Manually process insights, then summarize¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ae4aa5e-c666-4e90-990e-0aafee708748",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 92.6 ms, sys: 529 µs, total: 93.2 ms\n",
      "Wall time: 5min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "insights=[]\n",
    "for i in range(len(docs)):\n",
    "    insights.append(\n",
    "        insight_chain.invoke({\n",
    "        \"instructions\":\"Provide Key insights from the following text\",\n",
    "        \"document\": {docs[i].page_content}\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0e95f51-4b15-405e-834f-23dac661916d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "str_parser = StrOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    \n",
    "    Human:\n",
    "    {instructions} : \\\"{document}\\\"\n",
    "    Assistant:\"\"\",\n",
    "    input_variables=[\"instructions\",\"document\"]\n",
    ")\n",
    "\n",
    "summary_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c98af8c-531a-41e6-9dc0-dbe0c80f0e80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are the key insights from the provided text:\n",
      "\n",
      "Amazon has been using machine learning extensively for 25 years, employing it in everything from personalized ecommerce recommendations to fulfillment center pick paths, to drones for Prime Air, to Alexa, to the many machine learning services AWS offers. Large Language Models (“LLMs”) and Generative AI are core to setting Amazon up to invent in every area of our business for many decades to come. Machine learning has been a technology with high promise for several decades, but it’s only been the last five to ten years that it’s started to be used more pervasively by companies. AWS is offering the most price-performant machine learning chips in Trainium and Inferentia so small and large companies can afford to train and run their LLMs in production. LLMs and Generative AI are going to be a big deal for customers, our shareholders, and Amazon. While we have a consumer business that’s $434B in 2022, the vast majority of total market segment share in global retail still resides in physical stores (roughly 80%). And, it’s a similar story for Global IT spending, where we have AWS revenue of $80B in 2022, with about 90% of Global IT spending still on-premises and yet to migrate to the cloud. As these equations steadily flip—as we’re already seeing happen—we believe our leading customer experiences, relentless invention, customer focus, and hard work will result in significant growth in the coming years.\n",
      "CPU times: user 7.28 ms, sys: 175 µs, total: 7.46 ms\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(summary_chain.invoke({\n",
    "        \"instructions\":\"You will be provided with multiple sets of insights. Compile and summarize these insights and provide key takeaways in one concise paragraph. Do not use the original xml tags. Just provide a paragraph with your compiled insights.\",\n",
    "        \"document\": {'\\n'.join(insights)}\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cd7708-953f-4f59-a4da-65364e293c41",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "You have now experimented with using boto3 SDK which provides a vanilla exposure to Amazon Bedrock API. Using this API you have seen the use case of generating a summary of a Meeting and Earnings Call Transcripts using 2 different foundation models: entire output and streaming output generation.\n",
    "\n",
    "#### Take aways\n",
    "- Adapt this notebook to experiment with different models available through Amazon Bedrock such as Amazon Titan and AI21 Labs Jurassic models.\n",
    "- Change the prompts to your specific usecase and evaluate the output of different models.\n",
    "- Play with the token length to understand the latency and responsiveness of the service.\n",
    "- Apply different prompt engineering principles to get better outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c37c87-9b2c-43ea-b83a-4b2ce4a2d32d",
   "metadata": {},
   "source": [
    "### Restart Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb386925-f95e-4964-90e7-97f1e83b2bb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae4d270-1326-4384-a0b6-a8df91ef5920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
